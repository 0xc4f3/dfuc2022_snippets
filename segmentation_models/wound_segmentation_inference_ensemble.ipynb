{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install segmentation_models_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "from os import path\n",
    "from matplotlib import gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(smp.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = 'root_dir'\n",
    "DATASET_NAME = 'baseline'\n",
    "DATASET_PART = 'test'\n",
    "\n",
    "# images, labels (optional)\n",
    "DIR_IMAGES = path.join(ROOT_DIR, DATASET_NAME, DATASET_PART, 'images/')\n",
    "DIR_LABELS = None\n",
    "\n",
    "print('in:', DIR_IMAGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keyworded visualization: title_name_1=image1, title_name_2=image2, ...\n",
    "def visualize(**images):\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i+1)\n",
    "        plt.title(' '.join(name.split('_')).title()) # keyword as title\n",
    "        plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "# non-keyworded images: image1, image2, ...\n",
    "def visualize_grid(*images):\n",
    "    n, cols = len(images), 4\n",
    "    rows = int(np.ceil(n / cols))\n",
    "    gs = gridspec.GridSpec(rows, cols)\n",
    "    fig = plt.figure(figsize=(16, 4*rows))\n",
    "    fig.tight_layout()\n",
    "    for i in range(n):\n",
    "        ax = fig.add_subplot(gs[i])\n",
    "        ax.imshow(images[i])\n",
    "        ax.axis('off')    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset as BaseDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(BaseDataset):\n",
    "    \n",
    "    CLASSES = ['background', 'wound']\n",
    "    \n",
    "    def __init__(self, images_dir, masks_dir, classes=None, augmentation=None, preprocessing=None):\n",
    "        self.ids = os.listdir(images_dir)\n",
    "        self.images_fps = [path.join(images_dir, image_id) for image_id in self.ids]\n",
    "        self.masks_fps = None if masks_dir == None else [path.join(masks_dir, image_id) for image_id in self.ids]\n",
    "        \n",
    "        # convert str names to class values on masks\n",
    "        self.class_values = [self.CLASSES.index(cls.lower()) for cls in classes]\n",
    "        \n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        # read data\n",
    "        image = cv.imread(self.images_fps[i])\n",
    "        image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "        mask = None\n",
    "        \n",
    "        if self.masks_fps != None: \n",
    "            mask = cv.imread(self.masks_fps[i], 0)\n",
    "        \n",
    "            # extract certain classes from mask (e.g. wound)\n",
    "            masks = [(mask == v) for v in self.class_values]\n",
    "            mask = np.stack(masks, axis=-1).astype('float')\n",
    "        \n",
    "        # apply augmentations\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image)#, mask=mask)\n",
    "            image = sample['image']\n",
    "            mask = None if mask == None else sample['mask']\n",
    "        \n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image)#, mask=mask)\n",
    "            image = sample['image']\n",
    "            mask = None if mask == None else sample['mask']\n",
    "            \n",
    "        return image, mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# inspect data\n",
    "pred_dataset = Dataset(DIR_IMAGES, DIR_LABELS, classes=['wound'])\n",
    "image, mask = pred_dataset[0] # get some sample\n",
    "\n",
    "if mask is None:\n",
    "    print('No mask for image available')\n",
    "    visualize(image=image)    \n",
    "else:\n",
    "    visualize(image=image, wound_mask=mask.squeeze())\n",
    "    \n",
    "# save image height/width\n",
    "IMAGE_HEIGHT, IMAGE_WIDTH, _ = image.shape\n",
    "assert (IMAGE_HEIGHT % 32) + (IMAGE_WIDTH % 32) == 0, 'image height/width must be divisible by 32'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import albumentations as albu\n",
    "import segmentation_models_pytorch as smp\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_validation_augmentation():\n",
    "    test_transform = [\n",
    "        #albu.PadIfNeeded(IMAGE_HEIGHT, IMAGE_WIDTH) # padding for %32=0\n",
    "    ]    \n",
    "    return albu.Compose(test_transform)\n",
    "\n",
    "\n",
    "def to_tensor(x, **kwargs):    \n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "\n",
    "def get_preprocessing(preprocessing_fn):\n",
    "    _transform = [\n",
    "        albu.Lambda(image=preprocessing_fn),\n",
    "        albu.Lambda(image=to_tensor),\n",
    "        #albu.Lambda(image=to_tensor, mask=to_tensor),\n",
    "    ]    \n",
    "    return albu.Compose(_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "DIR_MODELS = 'models_dir'\n",
    "!ls $DIR_MODELS | grep tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAMES = [\n",
    "    \n",
    "    # id15, currently best performing base model ensemble\n",
    "    #'training___baseline___fpn_se_resnext101_32x4d_imagenet_sigmoid___adam_lr1e-04_lrd15e-05_lrd21e-05___medium___bs24_e150_ed1100_ed2135___sameconf1',\n",
    "    #'training___baseline___fpn_se_resnext101_32x4d_imagenet_sigmoid___adam_lr1e-04_lrd15e-05_lrd21e-05___medium___bs24_e150_ed1100_ed2135___sameconf2',\n",
    "    #'training___baseline___fpn_se_resnext101_32x4d_imagenet_sigmoid___adam_lr1e-04_lrd15e-05_lrd21e-05___medium___bs24_e150_ed1100_ed2135___sameconf3',\n",
    "    #'training___baseline___fpn_se_resnext101_32x4d_imagenet_sigmoid___adam_lr1e-04_lrd15e-05_lrd21e-05___medium___bs24_e150_ed1100_ed2135___sameconf4',\n",
    "    #'training___baseline___fpn_se_resnext101_32x4d_imagenet_sigmoid___adam_lr1e-04_lrd15e-05_lrd21e-05___medium___bs24_e150_ed1100_ed2135___sameconf5'\n",
    "    \n",
    "    # id22 (test id1)\n",
    "    #'training__baseline+tsynq95id15_4k__fpn_se_resnext101_32x4d_imagenet_sigmoid__adam_lr1e-04_lrd11e-05__medium__bs24_e150_ed1120__sameconf1',\n",
    "    #'training__baseline+tsynq95id15_4k__fpn_se_resnext101_32x4d_imagenet_sigmoid__adam_lr1e-04_lrd11e-05__medium__bs24_e150_ed1120__sameconf2',\n",
    "    #'training__baseline+tsynq95id15_4k__fpn_se_resnext101_32x4d_imagenet_sigmoid__adam_lr1e-04_lrd11e-05__medium__bs24_e150_ed1120__sameconf3',\n",
    "    #'training__baseline+tsynq95id15_4k__fpn_se_resnext101_32x4d_imagenet_sigmoid__adam_lr1e-04_lrd11e-05__medium__bs24_e150_ed1120__sameconf4',\n",
    "    #'training__baseline+tsynq95id15_4k__fpn_se_resnext101_32x4d_imagenet_sigmoid__adam_lr1e-04_lrd11e-05__medium__bs24_e150_ed1120__sameconf5',\n",
    "    \n",
    "    # id30 (test id2)\n",
    "    'training___baseline+ttsynq95id15_2k___fpn_se_resnext101_32x4d_imagenet_sigmoid___adam_lr1e-04_lrd11e-05___medium___bs24_e150_ed1120___sameconf1',\n",
    "    'training___baseline+ttsynq95id15_2k___fpn_se_resnext101_32x4d_imagenet_sigmoid___adam_lr1e-04_lrd11e-05___medium___bs24_e150_ed1120___sameconf2',\n",
    "    'training___baseline+ttsynq95id15_2k___fpn_se_resnext101_32x4d_imagenet_sigmoid___adam_lr1e-04_lrd11e-05___medium___bs24_e150_ed1120___sameconf3',\n",
    "    'training___baseline+ttsynq95id15_2k___fpn_se_resnext101_32x4d_imagenet_sigmoid___adam_lr1e-04_lrd11e-05___medium___bs24_e150_ed1120___sameconf4',\n",
    "    'training___baseline+ttsynq95id15_2k___fpn_se_resnext101_32x4d_imagenet_sigmoid___adam_lr1e-04_lrd11e-05___medium___bs24_e150_ed1120___sameconf5'\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODER = 'se_resnext101_32x4d'\n",
    "WEIGHTS = 'imagenet'\n",
    "CLASSES = ['wound']\n",
    "DEVICE = 'cuda'\n",
    "VISUALIZE = False\n",
    "\n",
    "DIR_PREDS = path.join(\n",
    "    'output_dir', \n",
    "    DATASET_NAME, DATASET_PART, MODEL_NAMES[0] + '__5x5_ensemble'\n",
    ")\n",
    "print('out:', DIR_PREDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regular last model ensemble\n",
    "#models = [torch.load(path.join(DIR_MODELS, mn, 'last_model.pth')) for mn in MODEL_NAMES]\n",
    "\n",
    "# polyak last model ensemble\n",
    "models = []\n",
    "epochs = 5\n",
    "for mn in MODEL_NAMES:\n",
    "    for e in range(0, epochs):\n",
    "        models.append(torch.load(path.join(DIR_MODELS, mn, 'last_model-{}.pth'.format(e))))\n",
    "\n",
    "# for otherwise composed ensembles\n",
    "#models = [\n",
    "    #torch.load(path.join(DIR_MODELS, MODEL_NAMES[0], 'best_model_dice.pth')),\n",
    "    #torch.load(path.join(DIR_MODELS, MODEL_NAMES[1], 'best_model_dice.pth')),\n",
    "    #torch.load(path.join(DIR_MODELS, MODEL_NAMES[2], 'best_model_dice_fold3.pth')),\n",
    "    #torch.load(path.join(DIR_MODELS, MODEL_NAMES[3], 'best_model_dice_fold4.pth')),\n",
    "    #torch.load(path.join(DIR_MODELS, MODEL_NAMES[4], 'best_model_dice_fold5.pth'))\n",
    "#]\n",
    "\n",
    "# IMPORTANT: MUST HAVE INSTALLED SMP VERSION UNDER WHICH MODELS WHERE CREATED TO LOAD THEM VIA TORCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dataset for image visualization (w/o transformations)\n",
    "pred_dataset_vis = Dataset(DIR_IMAGES, DIR_LABELS, classes=CLASSES)\n",
    "\n",
    "# test dataset for inference (w/ transformations)\n",
    "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, WEIGHTS)\n",
    "pred_dataset = Dataset(\n",
    "    DIR_IMAGES, DIR_LABELS, classes=CLASSES,\n",
    "    augmentation=get_validation_augmentation(),\n",
    "    preprocessing=get_preprocessing(preprocessing_fn)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SAMPLES   = range(0, len(pred_dataset))\n",
    "#SAMPLES   = range(0, 10)\n",
    "VISUALIZE = False\n",
    "WRITE     = True\n",
    "THRESHOLD = 0.5\n",
    "TTA       = False\n",
    "\n",
    "\n",
    "# create dir if not existing\n",
    "if WRITE and not path.isdir(DIR_PREDS):\n",
    "    # add inference parameters to prediction name\n",
    "    out = DIR_PREDS + f'__t{int(THRESHOLD*100)}_tta{1 if TTA else 0}'\n",
    "    \n",
    "    print('out:', out)\n",
    "    os.makedirs(out)  \n",
    "    \n",
    "# load model, predict\n",
    "for i in SAMPLES:\n",
    "    image_name = pred_dataset.ids[i]    \n",
    "    image_vis = pred_dataset_vis[i][0].astype('uint8')\n",
    "    image, gt_mask = pred_dataset[i]    \n",
    "    image_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
    "    \n",
    "    pr_prob = None\n",
    "    with torch.no_grad(): # eval mode, do not compute gradients\n",
    "        \n",
    "        # prediction \n",
    "        pr_probs = [m.predict(image_tensor).squeeze(0).cpu() for m in models] # standard torch.nn.Module.forward syntax        \n",
    "        if TTA:\n",
    "            for flip_dim in [(2,), (3,), (2,3)]: # tta\n",
    "                pr_probs_tta =[torch.flip(m(torch.flip(image_tensor, flip_dim)), flip_dim).squeeze(0).cpu() for m in models]\n",
    "                for p in pr_probs_tta:\n",
    "                    pr_probs.append(p)\n",
    "        pr_prob = torch.stack(pr_probs, 0).mean(0).squeeze(0).cpu()        \n",
    "    \n",
    "   \n",
    "    # threshold-based prob map cut\n",
    "    pr_mask = (pr_prob > THRESHOLD).numpy().astype(np.uint8) # 0 = background, 1 = wound\n",
    "\n",
    "    # visualize\n",
    "    if VISUALIZE:\n",
    "        if gt_mask is None:\n",
    "            visualize(image=image_vis, probability_map=pr_prob, predicted_mask=pr_mask)\n",
    "        else:\n",
    "            gt_mask = gt_mask.squeeze()\n",
    "            visualize(image=image_vis, ground_truth_mask=gt_mask, predicted_mask=pr_mask)\n",
    "    \n",
    "    # write\n",
    "    if WRITE:\n",
    "        image_path = path.join(out, image_name)\n",
    "        im = Image.fromarray(pr_mask).convert(\"L\") # grayscale\n",
    "        im.save(image_path)\n",
    "        print('Saved:', image_path)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
